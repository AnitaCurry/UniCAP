<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>UniCAP</title>
    <description>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</description>
    <link>http://yourdomain.com/UniCAP/</link>
    <atom:link href="http://yourdomain.com/UniCAP/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 12 Nov 2015 13:25:21 +0800</pubDate>
    <lastBuildDate>Thu, 12 Nov 2015 13:25:21 +0800</lastBuildDate>
    <generator>Jekyll v3.0.0</generator>
    
      <item>
        <title>Install UniCAP</title>
        <description>&lt;h1 id=&quot;install-unicap&quot;&gt;Install UniCAP&lt;/h1&gt;
&lt;p&gt;These are instructions for installing UniCAP.&lt;/p&gt;

&lt;h2 id=&quot;using-docker-image&quot;&gt;Using Docker Image&lt;/h2&gt;

&lt;p&gt;We provide an Docker image with the stable version of UniCAP installed. Users can deploy UniCAP using our Big Data Platform (BDP) directly with the associated Docker image.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://155.69.146.43/bdp/guest&quot;&gt;Using UniCAP in BDP&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;installing-from-source&quot;&gt;Installing From Source&lt;/h2&gt;

&lt;h3 id=&quot;span-stylecolor-greenrequirementsspan&quot;&gt;&lt;span style=&quot;color: Green&quot;&gt;Requirements&lt;/span&gt;&lt;/h3&gt;

&lt;h4 id=&quot;span-stylecolor-bluec-compilerspan&quot;&gt;&lt;span style=&quot;color: blue&quot;&gt;C++ Compiler&lt;/span&gt;&lt;/h4&gt;

&lt;p&gt;UniCAP needs a C++ compiler supporting c++11, such as gcc &amp;gt;= 4.7.2 (prefer &amp;gt;= 4.8) or llvm &amp;gt;= 3.4. You can update gcc via either downloading packages, or building from source.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1) Ubuntu 12.04&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install python-software-properties
sudo add-apt-repository ppa:ubuntu-toolchain-r/test
sudo apt-get update
sudo apt-get install gcc-4.8 g++-4.8
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin gcc-4.8 50
sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin g++-4.8 50
sudo update-alternatives --config gcc
sudo update-alternatives --config g++
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After that, check the gcc version via:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gcc --version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;2) Ubuntu 14.04&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ubuntu 14.04 contains gcc-4.8 in its system software system. 
Check the gcc version:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gcc --version 
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;span-stylecolor-blueboost-libraryspan&quot;&gt;&lt;span style=&quot;color: blue&quot;&gt;Boost Library&lt;/span&gt;&lt;/h4&gt;

&lt;p&gt;UniCAP needs boost library &amp;gt;= 1.54 (prefer &amp;gt;= 1.55).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1) Ubuntu 12.04&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You can install boost via ppa in ubuntu 12.04:&lt;/p&gt;

&lt;p&gt;Add ppa:boost-latest/ppa to your system’s software sources based on the instructions in&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://launchpad.net/~boost-latest/+archive/ubuntu/ppa&quot;&gt;https://launchpad.net/~boost-latest/+archive/ubuntu/ppa&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Update your system’s software sources:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get update
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install boost 1.55:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install libboost1.55-all-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;2) Ubuntu 14.04&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ubuntu 14.04 contains boost1.55&lt;/p&gt;

 	sudo apt-get install libboost1.55-all-dev

&lt;hr /&gt;

&lt;h4 id=&quot;span-stylecolor-bluethriftspan&quot;&gt;&lt;span style=&quot;color: blue&quot;&gt;Thrift&lt;/span&gt;&lt;/h4&gt;

&lt;p&gt;1) Install thrift dependencies:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install libevent-dev automake libtool
sudo apt-get install flex bison pkg-config libssl-dev  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2) Install thrift from source&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget http://mirror.nus.edu.sg/apache/thrift/0.9.2/thrift-0.9.2.tar.gz
tar zxvf thrift-0.9.2.tar.gz
cd thrift-0.9.2
./configure --without-java --without-go
make
sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;span-stylecolor-blueglog-and-gflagsspan&quot;&gt;&lt;span style=&quot;color: blue&quot;&gt;Glog and Gflags&lt;/span&gt;&lt;/h4&gt;

&lt;p&gt;UniCAP uses glog and gflags to print log data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1) Ubuntu 12.04&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Install Glog&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget https://google-glog.googlecode.com/files/glog-0.3.3.tar.gz
tar zxvf glog-0.3.3.tar.gz
cd glog-0.3.3
./configure
make
sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install Gflags&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget https://github.com/schuhschuh/gflags/archive/master.zip
unzip master.zip
cd gflags-master
mkdir build &amp;amp;&amp;amp; cd build
export CXXFLAGS=&quot;-fPIC&quot; &amp;amp;&amp;amp; cmake .. &amp;amp;&amp;amp; make VERBOSE=1
make
sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;2) Ubuntu 14.04&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;sudo apt-get install libgflags-dev libgoogle-glog-dev&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;span-stylecolor-blueleveldbspan&quot;&gt;&lt;span style=&quot;color: blue&quot;&gt;LevelDB&lt;/span&gt;&lt;/h4&gt;

&lt;p&gt;UniCAP uses LevelDB as one of its storage systems.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install libleveldb-dev libsnappy-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;span-stylecolor-blueopencv2span&quot;&gt;&lt;span style=&quot;color: blue&quot;&gt;OpenCV2&lt;/span&gt;&lt;/h4&gt;

&lt;p&gt;UniCAP uses OpenCV2 run image related applications.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget https://github.com/Itseez/opencv/archive/2.4.11.zip
unzip 2.4.11.zip
./configure
make 
sudo make install ---
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;span-stylecolor-bluempispan&quot;&gt;&lt;span style=&quot;color: blue&quot;&gt;MPI&lt;/span&gt;&lt;/h4&gt;

&lt;p&gt;UniCAP uses MPI to parallel the application.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install libopenmpi-dev openmpi-bin openmpi-common openmpi-doc
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;span-stylecolor-bluehdfsspan&quot;&gt;&lt;span style=&quot;color: blue&quot;&gt;HDFS&lt;/span&gt;&lt;/h4&gt;

&lt;p&gt;To work with Hadoop Distributed File System using C++, there are two choices:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1) libhdfs3&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Libhdfs3 a native c/c++ hdfs client (there is no need to use Java to work with HDFS in application layer).&lt;/p&gt;

&lt;p&gt;Refer to &lt;a href=&quot;https://github.com/PivotalRD/libhdfs3&quot;&gt;https://github.com/PivotalRD/libhdfs3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To compile libhdfs3, users need following libraries:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install protobuf-compiler  libprotobuf-dev 
sudo apt-get install libkrb5-dev libxml2-dev libuuid1 uuid-dev 
sudo apt-get install libgsasl7 libgsasl7-dev 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;2) libhdfs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Libhdfs is a JNI based C api for Hadoop’s DFS. It provides a simple subset of C apis to manipulate DFS files and the filesystem.&lt;/p&gt;

&lt;p&gt;Clients should compile libhdfs based on their HDFS version in use. To work with JNI, there are some modifications in system:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /etc/ld.so.conf.d/java.conf

append following configurations:

/opt/jdk1.7.0_75/jre/lib/amd64
/opt/jdk1.7.0_75/jre/lib/amd64/server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sometimes, the JNI-based application cannot find the needed jar packets for HDFS operations. Using following commands to generate a shell script, run it at the start up of the application:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;find /usr/lib/hadoop -name *.jar|awk &#39;{ printf(&quot;export CLASSPATH=%s:$CLASSPATH\n&quot;, $0); }&#39; &amp;gt;&amp;gt; hdfs_jni.sh
find /usr/lib/hadoop/lib -name *.jar|awk &#39;{ printf(&quot;export CLASSPATH=%s:$CLASSPATH\n&quot;, $0); }&#39; &amp;gt;&amp;gt; hdfs_jni.sh
find /usr/lib/hadoop-hdfs/ -name *.jar|awk &#39;{ printf(&quot;export CLASSPATH=%s:$CLASSPATH\n&quot;, $0); }&#39; &amp;gt;&amp;gt; hdfs_jni.sh
find /usr/lib/hadoop-hdfs/lib/ -name *.jar|awk &#39;{ printf(&quot;export CLASSPATH=%s:$CLASSPATH\n&quot;, $0); }&#39; &amp;gt;&amp;gt; hdfs_jni.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;span-stylecolor-greencompile-unicapspan&quot;&gt;&lt;span style=&quot;color: Green&quot;&gt;Compile UniCAP&lt;/span&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/sunpengsdu/unicap
cd unicap/build
cmake ..
make -j8
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Sat, 07 Nov 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/UniCAP/unicap/2015/11/07/install.html</link>
        <guid isPermaLink="true">http://yourdomain.com/UniCAP/unicap/2015/11/07/install.html</guid>
        
        
        <category>unicap</category>
        
      </item>
    
      <item>
        <title>index</title>
        <description>&lt;h1 id=&quot;span-stylecolor-4499eeunicap-a-unified-computing-engine-for-fast-data-processingspan&quot;&gt;&lt;span style=&quot;color: #4499ee&quot;&gt;UniCAP: A Unified Computing Engine for Fast Data Processing&lt;/span&gt;&lt;/h1&gt;
&lt;hr /&gt;

&lt;p&gt;The UniCAP project is a distributed computing engine for executing data parallel programmes, which consist of a complex of  directed-acyclic-graph (DAG) of tasks. It unifies &lt;em&gt;Batch Processing&lt;/em&gt; and &lt;em&gt;Stream Processing&lt;/em&gt; in one system using a &lt;strong&gt;timed dataflow model&lt;/strong&gt;. Clients can either use UniCAP in NTU &lt;a href=&quot;http://155.69.146.43/bdp&quot;&gt;Big Data Platform (BDP)&lt;/a&gt;, or build from source codes.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;    
&lt;img src=&quot;http://cap-ntu.github.io/UniCAP/img/unicap_sys.jpg&quot; width=&quot;450&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;The main design features of UniCAP are:&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;span-stylecolor-43a102timed-dataflowspan&quot;&gt;&lt;span style=&quot;color: #43a102&quot;&gt;Timed Dataflow&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Timed Dataflow&lt;/strong&gt; aims at reducing the communication overhead for iterative jobs with shared variables. In such jobs, the edge node reads input data and shared variables, and updates the shared variables at each iteration. This kind of application includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Supervised Machine Learning&lt;/em&gt;: To train a model, the supervised machine learning applications (e.g., logistic regression, artificial neural network, etc.) need to update the parameters, which are the shared variables, based on the input data and old parameters at each iteration.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Graph Processing&lt;/em&gt;: The graph processing jobs (e.g., single source shortest path, pagerank, etc.) usually update the graph nodes’ weights, which are the shared variables, based on the graph data and old weights at each iteration.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Precious approaches like Hadoop, Spark have high communication overhead to fetch all the shared variables at each iteration. Our experiments show that the shared variable query time can take up to &lt;strong&gt;60%&lt;/strong&gt; of total execution time of an iteration. However, our experiments also show that a large part of shared variables are &lt;strong&gt;static values&lt;/strong&gt; during the computation. However, previous approaches always fetch all the shared variables, even if there is no change.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Time Dataflow&lt;/strong&gt; tackles this problem by adding logical timestamp to the shared variables. Thus, at each iteration, the computation nodes (which caches the old shared variables in previous iteration) only need to fetch the changed shared variables rather than all the shared variables. Experiments show that &lt;strong&gt;timed dataflow&lt;/strong&gt; can accelerate logistic regression and pagerank 30% and 45% respectively, compared to Spark.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;    
&lt;img src=&quot;http://cap-ntu.github.io/UniCAP/img/timed_data_flow.jpg&quot; width=&quot;480&quot; /&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;span-stylecolor-43a102hybrid-vertexspan&quot;&gt;&lt;span style=&quot;color: #43a102&quot;&gt;Hybrid Vertex&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;UniCAP embodies a set of storage systems as its &lt;em&gt;Vertexes&lt;/em&gt;. Generally, UniCAP can support both pure-memory and mix memory-disk storage.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;    
&lt;img src=&quot;http://cap-ntu.github.io/UniCAP/img/unicap_storage.png&quot; width=&quot;400&quot; /&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;span-stylecolor-43a102hybrid-edge-in-developmentspan&quot;&gt;&lt;span style=&quot;color: #43a102&quot;&gt;Hybrid Edge (In Development)&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;UniCAP supports schedule tasks using both GPU and CPU in a single  application. Compared to other approaches, which can only use CPU or GPU, UniCAP maximize the system resource utilization.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;    
&lt;img src=&quot;http://cap-ntu.github.io/UniCAP/img/unicap_gpu.png&quot; width=&quot;400&quot; /&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 07 Nov 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/UniCAP/unicap/2015/11/07/index.html</link>
        <guid isPermaLink="true">http://yourdomain.com/UniCAP/unicap/2015/11/07/index.html</guid>
        
        
        <category>unicap</category>
        
      </item>
    
      <item>
        <title>Getting Started</title>
        <description>&lt;h1 id=&quot;span-stylecolor-4499eegetting-startedspan&quot;&gt;&lt;span style=&quot;color: #4499ee&quot;&gt;Getting Started&lt;/span&gt;&lt;/h1&gt;
&lt;hr /&gt;

&lt;div align=&quot;center&quot;&gt;    
&lt;img src=&quot;http://cap-ntu.github.io/UniCAP/img/unicap_start.jpg&quot; width=&quot;400&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;span-stylecolor-43a102define-the-vertexspan&quot;&gt;&lt;span style=&quot;color: #43a102&quot;&gt;Define the Vertex&lt;/span&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ntu::cap::DAG::create_table (table_name, shard_num, partition_algo)
ntu::cap::DAG::create_cf&amp;lt;T&amp;gt; (table_name, cf_name, cf_type)
ntu::cap::DAG::load_from_hdfs (hdfs_path, table_name, cf_name)
ntu::cap::DAG::load_local_file (local_path, table_name, cf_name)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;span-stylecolor-43a102define-the-edge-inputoutput-pathspan&quot;&gt;&lt;span style=&quot;color: #43a102&quot;&gt;Define the Edge (Input/Output Path)&lt;/span&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;ntu::cap::Storage::vector_put&amp;lt;T&amp;gt; 
ntu::cap::Storage::vector_get&amp;lt;T&amp;gt;
ntu::cap::Storage::scan&amp;lt;T&amp;gt;
ntu::cap::Storage::timed_put&amp;lt;T&amp;gt; 
ntu::cap::Storage::timed_scan&amp;lt;T&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;span-stylecolor-43a102define-the-edge-computing-functionsspan&quot;&gt;&lt;span style=&quot;color: #43a102&quot;&gt;Define the Edge (Computing Functions)&lt;/span&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;class UCPUFunctions : public CPUFunctions {
public:
    static UCPUFunctions&amp;amp; singleton() {
        static UCPUFunctions u_cpu_function;
        return u_cpu_function;
    }

    UCPUFunctions() : CPUFunctions() {
        CPUFunctions::_cpu_functions_p[&quot;User_functions&quot;] = hello_world;
    }

    static int64_t User_functions (TaskNode new_task) {
        //User implementation
        return 1;
    }
}
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Sat, 07 Nov 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/UniCAP/unicap/2015/11/07/getting-started.html</link>
        <guid isPermaLink="true">http://yourdomain.com/UniCAP/unicap/2015/11/07/getting-started.html</guid>
        
        
        <category>unicap</category>
        
      </item>
    
      <item>
        <title>Example</title>
        <description>&lt;h1 id=&quot;span-stylecolor-4499ee-examplespan&quot;&gt;&lt;span style=&quot;color: #4499ee&quot;&gt; Example&lt;/span&gt;&lt;/h1&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;span-stylecolor-43a102wordcountspan&quot;&gt;&lt;span style=&quot;color: #43a102&quot;&gt;WordCount&lt;/span&gt;&lt;/h3&gt;

&lt;div align=&quot;center&quot;&gt;    
&lt;img src=&quot;http://cap-ntu.github.io/UniCAP/img/unicap_wordcount.jpg&quot; width=&quot;500&quot; /&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Define the Vertexes&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;DAG::load_hdfs_file(&quot;/dataset/wikipedia_300GB&quot;, 
	&quot;word_count&quot;,
	&quot;data_set&quot;,
	8*1024*1024);

KeyPartition word_count_inter;
word_count_inter.__set_partition_algo(KeyPartitionAlgo::HashingPartition);

DAG::create_table(&quot;word_count_result&quot;, 50, word_count_inter);

DAG::create_cf(&quot;word_count_result&quot;, 
	&quot;inner_result&quot;, 
	StorageType::InMemoryKeyValue, 
	ValueType::Int64);
	
DAG::create_cf(&quot;word_count_result&quot;, 
	&quot;final_result&quot;, 
	StorageType::InMemoryKeyValue, 
	ValueType::Int64);
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;Define the Edges&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;std::shared_ptr&amp;lt;Stage&amp;gt;stage_map = std::shared_ptr&amp;lt;Stage&amp;gt;(new Stage());
stage_map-&amp;gt;set_function_name(&quot;word_count_map&quot;);
std::vector&amp;lt;std::string&amp;gt; src_cf;
src_cf.push_back(&quot;data_set&quot;);
stage_map-&amp;gt;set_src(&quot;word_count&quot;, src_cf);
stage_map-&amp;gt;set_dst(&quot;word_count_result&quot;, &quot;inner_result&quot;);
Scheduler::singleton().push_back(stage_map);

std::shared_ptr&amp;lt;Stage&amp;gt;stage_reduce = std::shared_ptr&amp;lt;Stage&amp;gt;(new Stage());
stage_reduce-&amp;gt;set_function_name(&quot;word_count_reduce&quot;);
std::vector&amp;lt;std::string&amp;gt; dst_cf;
dst_cf.push_back(&quot;inner_result&quot;);
stage_reduce-&amp;gt;set_src(&quot;word_count_result&quot;, dst_cf);
stage_reduce-&amp;gt;set_dst(&quot;word_count_result&quot;, &quot;final_result&quot;);
Scheduler::singleton().push_back(stage_reduce);
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Sat, 07 Nov 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/UniCAP/unicap/2015/11/07/example.html</link>
        <guid isPermaLink="true">http://yourdomain.com/UniCAP/unicap/2015/11/07/example.html</guid>
        
        
        <category>unicap</category>
        
      </item>
    
      <item>
        <title>Deveop</title>
        <description>&lt;p&gt;TO DO&lt;/p&gt;
</description>
        <pubDate>Sat, 07 Nov 2015 00:00:00 +0800</pubDate>
        <link>http://yourdomain.com/UniCAP/unicap/2015/11/07/document.html</link>
        <guid isPermaLink="true">http://yourdomain.com/UniCAP/unicap/2015/11/07/document.html</guid>
        
        
        <category>unicap</category>
        
      </item>
    
  </channel>
</rss>
